# ì¼íšŒì„± ì½”ë“œ -> ì£¼ì„ í•´ì œí•˜ì§€ ë§ˆì„¸ìš”.
'''
import os
import faiss
from sentence_transformers import SentenceTransformer
import pandas as pd
import numpy as np
import pickle
from tqdm import tqdm
import shutil
from datetime import datetime

def backup_existing_files():
    """ê¸°ì¡´ íŒŒì¼ì„ ë°±ì—…í•˜ëŠ” í•¨ìˆ˜"""
    base_path = ".."
    files_to_backup = ["faiss_index.jobkorea", "faiss_qa_mapping.pkl"]
    backup_dir = os.path.join(base_path, "backups")
    
    # ë°±ì—… ë””ë ‰í† ë¦¬ ìƒì„±
    if not os.path.exists(backup_dir):
        os.makedirs(backup_dir)
    
    # í˜„ì¬ ì‹œê°„ìœ¼ë¡œ ë°±ì—… íŒŒì¼ëª… ìƒì„±
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    for file in files_to_backup:
        source_path = os.path.join(base_path, file)
        if os.path.exists(source_path):
            backup_path = os.path.join(backup_dir, f"{file}.{timestamp}")
            shutil.copy2(source_path, backup_path)
            print(f"âœ… {file} ë°±ì—… ì™„ë£Œ: {backup_path}")

# CSV íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
df = pd.read_csv('../data/jobkorea.csv')

# wide í˜•íƒœë¥¼ tidy í˜•íƒœë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
def data_preprocess():
    qa_pairs = []
    for idx, row in df.iterrows():
        for i in range(1, 20):  # question1 ~ question19ê¹Œì§€
            q_col = f'question{i}'
            a_col = f'answer{i}'
            if pd.notna(row.get(q_col)) and pd.notna(row.get(a_col)):
                qa_pairs.append({
                    'question': row[q_col],
                    'answer': row[a_col]
                })
    return pd.DataFrame(qa_pairs)

def process_batch(model, questions, batch_size=1000):
    """ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ í•¨ìˆ˜"""
    embeddings = []
    for i in tqdm(range(0, len(questions), batch_size), desc="Processing batches"):
        batch = questions[i:i+batch_size]
        batch_embeddings = model.encode(batch, show_progress_bar=False)
        embeddings.append(batch_embeddings)
    return np.vstack(embeddings)

def main():
    # 0. ê¸°ì¡´ íŒŒì¼ ë°±ì—…
    print("ê¸°ì¡´ íŒŒì¼ ë°±ì—… ì¤‘...")
    backup_existing_files()
    
    # 1. ë°ì´í„° ì „ì²˜ë¦¬
    print("ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...")
    qa_df = data_preprocess()
    print(f"ì „ì²˜ë¦¬ ì™„ë£Œ. ì´ {len(qa_df)}ê°œì˜ QA ìŒ ìƒì„±")

    # 2. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
    print("ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
    model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')

    # 3. ì§ˆë¬¸ ì„ë² ë”© (ë°°ì¹˜ ì²˜ë¦¬)
    print("ì§ˆë¬¸ ì„ë² ë”© ìƒì„± ì¤‘...")
    questions = qa_df['question'].tolist()
    question_embeddings = process_batch(model, questions)
    print(f"ì„ë² ë”© ìƒì„± ì™„ë£Œ. ì°¨ì›: {question_embeddings.shape}")

    # 4. FAISS ì¸ë±ìŠ¤ ìƒì„± (cosine similarityë¥¼ ìœ„í•œ L2 normalize)
    print("FAISS ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
    dimension = question_embeddings.shape[1]
    index = faiss.IndexIVFFlat(faiss.IndexFlatL2(dimension), dimension, 100)
    index.train(question_embeddings)
    print("ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")

    # 5. ì¸ë±ìŠ¤ì™€ ë§¤í•‘ ì •ë³´ë¥¼ ì €ì¥
    print("ê²°ê³¼ ì €ì¥ ì¤‘...")
    faiss.write_index(index, "./faiss_index.jobkorea")  # ë²¡í„° ì¸ë±ìŠ¤ -> ì¸ë±ìŠ¤ë¡œ ì•„ë˜ ì§ˆë¬¸/ë‹µë³€ì´ë‘ ì—°ê²°ì§€ì–´ì•¼í•¨
    with open("./faiss_qa_mapping.pkl", "wb") as f:  # ì§ˆë¬¸/ë‹µë³€ ë§¤í•‘ ì •ë³´
        pickle.dump(qa_df.to_dict(orient='records'), f)
    print("ì €ì¥ ì™„ë£Œ")

    print("âœ… ì„ë² ë”© ë° FAISS ì¸ë±ì‹± ì™„ë£Œ!")

# ğŸ”¥ ì‹¤í–‰ íŠ¸ë¦¬ê±°
if __name__ == "__main__":
    main()
'''