import os
import pandas as pd
import numpy as np
from PyPDF2 import PdfReader
import openai
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain_openai import OpenAI
from routers.pdf_storage import pdf_storage
from config import FILE_DIR, API_KEY
from db import SessionLocal, InterviewSessionDB, ChatMessageDB
from typing import Optional, Dict, Any
import logging
import re
from sentence_transformers import SentenceTransformer
import faiss
import pickle

logger = logging.getLogger(__name__)

# FAISS íŒŒì¼ ê²½ë¡œ ì„¤ì •
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
FAISS_INDEX_PATH = os.path.join(BASE_DIR, "faiss_index.jobkorea")
FAISS_MAPPING_PATH = os.path.join(BASE_DIR, "faiss_qa_mapping.pkl")

class InterviewSession:
    def __init__(self, token: str, question_num=5, answer_per_question=5, mock_data_path=None):
        self.token = token
        self.llm = OpenAI(api_key=API_KEY, temperature=0.7)
        
        # PDF ë°ì´í„° ë¡œë“œ
        pdf_data = pdf_storage.get_pdf(token)
        if not pdf_data:
            raise ValueError(f"í† í° {token}ì— í•´ë‹¹í•˜ëŠ” PDF ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        self.resume = pdf_data.get("resume_text", "")
        self.recruit_url = pdf_data.get("recruitUrl", "")
        
        if not self.resume or not self.recruit_url:
            raise ValueError("ì´ë ¥ì„œ ë˜ëŠ” ì±„ìš©ê³µê³  URLì´ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
        self.current_main = 0
        self.current_follow_up = 0
        self.current_answer = 0
        self.question_num = question_num
        self.answer_per_question = answer_per_question
        self.main_questions = []
        self.follow_up_questions = [[] for _ in range(question_num)]
        self.answers = [[] for _ in range(question_num)]
        self.hints = [[] for _ in range(question_num)]
        self.feedbacks = [[] for _ in range(question_num)]
        
        self.mock_data_path = mock_data_path
        self.example_questions = self._load_mock_interview_data(mock_data_path)
        
        # FAISS ê´€ë ¨ ì´ˆê¸°í™”
        self._init_faiss()

    def _init_faiss(self):
        """FAISS ê´€ë ¨ ì´ˆê¸°í™”"""
        try:
            if not os.path.exists(FAISS_INDEX_PATH) or not os.path.exists(FAISS_MAPPING_PATH):
                raise FileNotFoundError("FAISS ì¸ë±ìŠ¤ ë˜ëŠ” ë§¤í•‘ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            
            self.index = faiss.read_index(FAISS_INDEX_PATH)
            with open(FAISS_MAPPING_PATH, "rb") as f:
                self.mapping = pickle.load(f)
            
            logger.info("âœ… FAISS ì¸ë±ìŠ¤ ë° ë§¤í•‘ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"FAISS ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            raise

    def _load_mock_interview_data(self, mock_data_path=None):
        if not mock_data_path:
            mock_data_path = os.path.join(FILE_DIR, "mock_interview_data.json")
        
        try:
            if os.path.exists(mock_data_path):
                df = pd.read_json(mock_data_path)
                return "\n".join(df['question'].tolist()[:5])
            else:
                return """
                í”„ë¡œì íŠ¸ì—ì„œ ê°€ì¥ í° ë„ì „ ê³¼ì œëŠ” ë¬´ì—‡ì´ì—ˆë‚˜ìš”?
                íŒ€ í”„ë¡œì íŠ¸ì—ì„œ ê°ˆë“±ì´ ë°œìƒí–ˆì„ ë•Œ ì–´ë–»ê²Œ í•´ê²°í•˜ì…¨ë‚˜ìš”?
                ê¸°ìˆ  ìŠ¤íƒì„ ì„ íƒí•œ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?
                í”„ë¡œì íŠ¸ì—ì„œ ë³¸ì¸ì˜ ì—­í• ì€ ë¬´ì—‡ì´ì—ˆë‚˜ìš”?
                ê°€ì¥ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œí•œ í”„ë¡œì íŠ¸ëŠ” ë¬´ì—‡ì¸ê°€ìš”?
                """
        except Exception as e:
            print(f"ëª¨ì˜ ë©´ì ‘ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {str(e)}")
            return ""
        
    # RAG ì‹œì‘ë¶€ë¶„ -> ë²¡í„° ì¸ë±ìŠ¤, ë§¤í•‘ ì •ë³´ ê°€ì ¸ì˜¤ê¸°    
    def _load_faiss_index(self):
        # ë²¡í„° ì¸ë±ìŠ¤ì™€ ë§¤í•‘ ì •ë³´ ë¡œë“œ
        index = faiss.read_index("../faiss_index.jobkorea")
        with open("../faiss_qa_mapping.pkl", "rb") as f:
            mapping = pickle.load(f)
        return index, mapping

    async def generate_main_questions(self, num_questions: int = 5):
        try:
            if self.main_questions:
                logger.info("ì´ë¯¸ ìƒì„±ëœ ì§ˆë¬¸ì´ ìˆìŠµë‹ˆë‹¤.")
                return self.main_questions

            logger.info("ğŸ¯ [generate_main_questions] ëŒ€í‘œ ì§ˆë¬¸ ìƒì„± ì‹œì‘")

        # 1. RAG ê¸°ë°˜ ìš°ì„  ì‹œë„
            try:
                query_text = f"{self.resume[:1000]} {self.recruit_url[:500]}"
                model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
                query_embedding = model.encode([query_text])
                faiss.normalize_L2(query_embedding)

                index = faiss.read_index("../faiss_index.jobkorea")
                with open("../faiss_qa_mapping.pkl", "rb") as f:
                    mapping = pickle.load(f)

                top_k = min(10, len(mapping))
                distances, indices = index.search(np.array(query_embedding), top_k)
                retrieved_questions = [mapping[i]['question'] for i in indices[0]]

                logger.info(f"ğŸ“¥ ìœ ì‚¬ ì§ˆë¬¸ {len(retrieved_questions)}ê°œ ì¶”ì¶œë¨")

                # LLM ì •ì œ
                prompt = PromptTemplate(
                    template=self._get_rag_question_template(),
                    input_variables=['retrieved_questions', 'resume', 'recruit_url']
                )
                chain = LLMChain(prompt=prompt, llm=self.llm)
                response = await chain.ainvoke({
                    'retrieved_questions': "\n".join(retrieved_questions),
                    'resume': self.resume[:1000],
                    'recruit_url': self.recruit_url[:500]
                })

                questions = [q.strip() for q in response.get('text', '').split('\n') if q.strip()]
                if questions:
                    self.main_questions = questions[:num_questions]
                    logger.info(f"âœ… RAG ê¸°ë°˜ ëŒ€í‘œì§ˆë¬¸ {len(self.main_questions)}ê°œ ìƒì„± ì™„ë£Œ")
                    return self.main_questions
                else:
                    logger.warning("ğŸ“­ RAG ê¸°ë°˜ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨, í”„ë¡¬í”„íŠ¸ ë°©ì‹ìœ¼ë¡œ ëŒ€ì²´")

            except Exception as e:
                logger.warning(f"â— RAG ì‹¤íŒ¨ â†’ í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ìœ¼ë¡œ ì „í™˜: {str(e)}")

            # 2. Fallback: ê¸°ì¡´ í”„ë¡¬í”„íŠ¸ ë°©ì‹
            prompt = PromptTemplate(
                template=self._get_question_template(),
                input_variables=['resume', 'recruit_url', 'example_questions']
            )
            
            # LLMChain ìƒì„± ë° ì‹¤í–‰
            chain = LLMChain(prompt=prompt, llm=self.llm)
            response = await chain.ainvoke({
                'resume': self.resume[:1000],
                'recruit_url': self.recruit_url,
                'example_questions': self.example_questions
            })

            questions_text = response.get('text', '').strip()
            if not questions_text:
                logger.error("âš ï¸ í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ ì‘ë‹µë„ ì‹¤íŒ¨")
                raise ValueError("ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨")

            questions = [q.strip() for q in questions_text.split('\n') if q.strip()]
            self.main_questions = questions[:num_questions]
            logger.info(f"âœ… í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ ëŒ€í‘œì§ˆë¬¸ {len(self.main_questions)}ê°œ ìƒì„± ì™„ë£Œ")
            return self.main_questions

        except Exception as e:
            logger.error(f"âŒ ëŒ€í‘œì§ˆë¬¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise ValueError("ëŒ€í‘œì§ˆë¬¸ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")


    async def generate_main_question(self):
        """ì €ì¥ëœ ëŒ€í‘œ ì§ˆë¬¸ì„ í•˜ë‚˜ì”© ë°˜í™˜í•˜ëŠ” ë©”ì„œë“œ"""
        try:
            # ì§ˆë¬¸ì´ ì—†ìœ¼ë©´ ìƒì„±
            if not self.main_questions:
                logger.info("ëŒ€í‘œì§ˆë¬¸ ìƒˆë¡œ ìƒì„±")
                self.main_questions = await self.generate_main_questions()
                if not self.main_questions:
                    logger.error("ëŒ€í‘œì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨")
                    return None
            
            # í˜„ì¬ ëŒ€í‘œì§ˆë¬¸ ì¸ë±ìŠ¤ í™•ì¸
            if self.current_main >= len(self.main_questions):
                logger.info("ëª¨ë“  ëŒ€í‘œì§ˆë¬¸ì´ ë°˜í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.")
                # ì„¸ì…˜ ì™„ë£Œ ì—¬ë¶€ í™•ì¸
                await self.check_session_completion()
                return None

            # í˜„ì¬ ì§ˆë¬¸ ë°˜í™˜ (ì¸ë±ì‹± ì œê±°)
            question = self.main_questions[self.current_main]
            # ì¸ë±ì‹± ì œê±° (ì˜ˆ: "1. ", "2. " ë“±)
            question = re.sub(r'^\d+\.\s*', '', question)
            logger.info(f"ëŒ€í‘œì§ˆë¬¸ {self.current_main + 1} ë°˜í™˜: {question}")
            
            return question
            
        except Exception as e:
            logger.error(f"ì§ˆë¬¸ ë°˜í™˜ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            raise ValueError("ëŒ€í‘œì§ˆë¬¸ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # ê¼¬ë¦¬ì§ˆë¬¸ ìƒì„±
    async def generate_follow_up(self, last_answer: str):
        """ì‚¬ìš©ì ë‹µë³€ì„ ë°”íƒ•ìœ¼ë¡œ ê¼¬ë¦¬ì§ˆë¬¸ì„ ìƒì„±í•˜ëŠ” ë©”ì„œë“œ"""
        try:
            # í˜„ì¬ ëŒ€í‘œì§ˆë¬¸ì— ëŒ€í•œ ê¼¬ë¦¬ì§ˆë¬¸ ê°œìˆ˜ í™•ì¸
            if len(self.follow_up_questions[self.current_main]) >= self.answer_per_question - 1:
                logger.warning(f"ë” ì´ìƒì˜ ê¼¬ë¦¬ì§ˆë¬¸ì´ ì—†ìŠµë‹ˆë‹¤. (ëŒ€í‘œì§ˆë¬¸ {self.current_main + 1})")
                return "ë” ì´ìƒì˜ ê¼¬ë¦¬ì§ˆë¬¸ì´ ì—†ìŠµë‹ˆë‹¤."
            
            # í˜„ì¬ ëŒ€í‘œì§ˆë¬¸ ê°€ì ¸ì˜¤ê¸°
            current_main_question = self.main_questions[self.current_main] if self.main_questions else ""
            
            # í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
            prompt = PromptTemplate(
                template=self._get_follow_up_template(),
                input_variables=['main_question', 'answer', 'previous_follow_ups']
            )
            
            # ì´ì „ ê¼¬ë¦¬ì§ˆë¬¸ë“¤ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            previous_follow_ups = "\n".join(self.follow_up_questions[self.current_main])
            
            # LLMChain ìƒì„± ë° ì‹¤í–‰
            chain = LLMChain(prompt=prompt, llm=self.llm)
            response = await chain.ainvoke({
                'main_question': current_main_question,
                'answer': last_answer,
                'previous_follow_ups': previous_follow_ups
            })
            
            if not response or not isinstance(response, dict):
                logger.error(f"ì˜ëª»ëœ ì‘ë‹µ í˜•ì‹: {response}")
                return "ê¼¬ë¦¬ì§ˆë¬¸ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            # ì‘ë‹µì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì •ì œ
            question = response.get('text', '').strip()
            if not question:
                logger.error("ì‘ë‹µì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return "ê¼¬ë¦¬ì§ˆë¬¸ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            # DBì— ì €ì¥
            db = SessionLocal()
            try:
                session = db.query(InterviewSessionDB).filter_by(session_token=self.token).first()
                if not session:
                    logger.error(f"ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {self.token}")
                    return "ê¼¬ë¦¬ì§ˆë¬¸ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

                new_follow_up = ChatMessageDB(
                    session_id=session.id,
                    message_type="follow_up",
                    content=question
                )
                db.add(new_follow_up)
                db.commit()
                db.refresh(new_follow_up)
                
                # ê¼¬ë¦¬ì§ˆë¬¸ ì €ì¥
                self.follow_up_questions[self.current_main].append(question)
                logger.info(f"ê¼¬ë¦¬ì§ˆë¬¸ ìƒì„± ì„±ê³µ (ëŒ€í‘œì§ˆë¬¸ {self.current_main + 1}): {question}")
                return question
                
            except Exception as e:
                logger.error(f"DB ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                db.rollback()
                return "ê¼¬ë¦¬ì§ˆë¬¸ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            finally:
                db.close()

        except Exception as e:
            logger.error(f"ê¼¬ë¦¬ì§ˆë¬¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return "ê¼¬ë¦¬ì§ˆë¬¸ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    def store_user_answer(self, session_id: int, answer: str):
        """ì‚¬ìš©ìì˜ ë‹µë³€ì„ ì €ì¥í•©ë‹ˆë‹¤."""
        try:
            # í˜„ì¬ ì§ˆë¬¸ ì¸ë±ìŠ¤ì— ë‹µë³€ ì €ì¥
            if 0 <= self.current_main < len(self.answers):
                self.answers[self.current_main].append(answer)
                print(f"âœ… ë‹µë³€ ì €ì¥ ì™„ë£Œ - ì§ˆë¬¸ {self.current_main + 1}")
            else:
                print(f"âš ï¸ ë‹µë³€ ì €ì¥ ì‹¤íŒ¨ - ìœ íš¨í•˜ì§€ ì•Šì€ ì§ˆë¬¸ ì¸ë±ìŠ¤: {self.current_main}")
        except Exception as e:
            print(f"âŒ ë‹µë³€ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise

    async def generate_hint(self, question, question_index):
        try:
            print(f"íŒíŠ¸ ìƒì„± ì‹œì‘ - ì§ˆë¬¸ ì¸ë±ìŠ¤: {question_index}, ì§ˆë¬¸: {question}")
            
            # ì´ë ¥ì„œ í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ
            resume_text = self.resume[:1000] if len(self.resume) > 1000 else self.resume
            
            prompt = PromptTemplate(
                template=self._get_hint_template(),
                input_variables=['resume', 'question', 'question_index']
            )
            chain = LLMChain(prompt=prompt, llm=self.llm)
            
            hint = chain.run({
                'resume': resume_text,
                'question': question,
                'question_index': question_index
            })
            
            if not hint:
                raise ValueError(f"ì§ˆë¬¸ {question_index}ì— ëŒ€í•œ íŒíŠ¸ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            hint = hint.strip()
            print(f"ìƒì„±ëœ íŒíŠ¸ (ì§ˆë¬¸ {question_index}): {hint}")
            
            # íŒíŠ¸ë¥¼ ì„¸ì…˜ì˜ íŒíŠ¸ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥
            if 0 <= question_index < len(self.hints):
                self.hints[question_index].append(hint)
            
            return hint
            
        except Exception as e:
            print(f"íŒíŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ì§ˆë¬¸ {question_index}): {str(e)}")
            return f"ì§ˆë¬¸ {question_index}ì— ëŒ€í•œ íŒíŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    async def generate_feedback(self, last_answer: str):
        """ì‚¬ìš©ì ë‹µë³€ì— ëŒ€í•œ í”¼ë“œë°±ì„ ìƒì„±í•˜ëŠ” ë©”ì„œë“œ"""
        try:
            # í˜„ì¬ ëŒ€í‘œì§ˆë¬¸ ê°€ì ¸ì˜¤ê¸°
            current_main_question = self.main_questions[self.current_main] if self.main_questions else ""
            
            # ì´ì „ í”¼ë“œë°±ë“¤ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            previous_feedbacks = "\n".join(self.feedbacks[self.current_main])
            
            # í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
            prompt = PromptTemplate(
                template=self._get_feedback_template(),
                input_variables=['main_question', 'answer', 'previous_feedbacks']
            )
            
            # LLMChain ìƒì„± ë° ì‹¤í–‰
            chain = LLMChain(prompt=prompt, llm=self.llm)
            response = await chain.ainvoke({
                'main_question': current_main_question,
                'answer': last_answer,
                'previous_feedbacks': previous_feedbacks
            })
            
            if not response or not isinstance(response, dict):
                logger.error(f"ì˜ëª»ëœ ì‘ë‹µ í˜•ì‹: {response}")
                return "í”¼ë“œë°±ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            # ì‘ë‹µì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì •ì œ
            feedback = response.get('text', '').strip()
            if not feedback:
                logger.error("ì‘ë‹µì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return "í”¼ë“œë°±ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            # DBì— ì €ì¥
            db = SessionLocal()
            try:
                session = db.query(InterviewSessionDB).filter_by(session_token=self.token).first()
                if not session:
                    logger.error(f"ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {self.token}")
                    return "í”¼ë“œë°±ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

                new_feedback = ChatMessageDB(
                    session_id=session.id,
                    message_type="feedback",
                    content=feedback
                )
                db.add(new_feedback)
                db.commit()
                db.refresh(new_feedback)
                    
                # í”¼ë“œë°± ì €ì¥
                self.feedbacks[self.current_main].append(feedback)
                logger.info(f"í”¼ë“œë°± ìƒì„± ì„±ê³µ (ëŒ€í‘œì§ˆë¬¸ {self.current_main + 1}): {feedback}")
                return feedback
                
            except Exception as e:
                logger.error(f"DB ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                db.rollback()
                return "í”¼ë“œë°±ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            finally:
                db.close()

        except Exception as e:
            logger.error(f"í”¼ë“œë°± ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return "í”¼ë“œë°±ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."


    def _get_rag_question_template(self):
        return '''
        You are an expert AI job interviewer. Based on the retrieved similar questions and the candidate's resume, generate interview questions.

        [Retrieved Similar Questions]
        {retrieved_questions}

        [Resume]
        {resume}

        [Job Description]
        {recruit_url}

        Requirements:
        1. Generate questions in Korean.
        2. Questions should be specific and tailored to the resume.
        3. Questions should be similar in style to the retrieved questions.
        4. Write only the questions, without additional explanations.
        5. Do not add numbering or bullet points.
        6. Each question should be on a new line.
        7. Generate at least 5 questions.
        '''

    def _get_question_template(self):
        return f'''
        ë‹¤ìŒ ì´ë ¥ì„œì™€ ì±„ìš© ê³µê³ ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë©´ì ‘ ì§ˆë¬¸ì„ ìƒì„±í•´ì£¼ì„¸ìš”.

        [ì´ë ¥ì„œ]
        {self.resume[:1000]}  # ì´ë ¥ì„œ í…ìŠ¤íŠ¸ë¥¼ 1000ìë¡œ ì œí•œ

        [ì±„ìš© ê³µê³ ]
        {self.recruit_url[:500]}  # ì±„ìš© ê³µê³  URLì„ 500ìë¡œ ì œí•œ

        [ì˜ˆì‹œ ì§ˆë¬¸]
        {self.example_questions}

        ìš”êµ¬ì‚¬í•­:
        1. Be in Korean.
        2. Be specific and tailored to the resume.
        3. Be similar to the example questions.
        4. Write only the question, without additional explanations or comments.
        5. Do not add numbering or indexing to the questions (like "1. ", "2. ", etc.).
        6. Each question should be on a new line.
        7. Do not add any numbering or bullet points.
        '''

    def _get_follow_up_template(self):
        return '''
        You are an expert AI job interviewer.

        [Current Main Question]
        {main_question}

        [Candidate's Answer]
        {answer}

        [Previous Follow-up Questions]
        {previous_follow_ups}
        
        Create a follow-up question that:
        1. Be in Korean.
        2. Must be different from the previous follow-up questions.
        3. Must be specifically related to the candidate's answer and the main question.
        4. Focus on:
           - Specific examples or situations mentioned
           - Technical details or methodologies used
           - Challenges faced and solutions implemented
           - Results and impacts achieved
        5. Be concise and direct.
        6. Only provide the follow-up question, without any additional explanations or comments.
        '''

    def _get_hint_template(self):       # íŒíŠ¸ ìƒì„± í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        return f'''
        [Resume Context]
        {self.resume}

        [Current Question]
        {{question}}

        [Guidelines]
        1. Extract three key technical keywords to emphasize in the response.  
        2. Suggest applying the STAR (Situation-Task-Action-Result) method.  
        3. Recommend using specific numbers/data.  
        4. Provide advice summarized in no more than two sentences.
        5. Be in Korean.
        6. Be specific and tailored to the resume.
        7. Only one question at a time.
        '''

    def _get_feedback_template(self):
        return '''
        You are an expert Korean career coach.  
        Based on the provided question and answer, write professional and helpful interview feedback in Korean.

        [ì§ˆë¬¸]
        {main_question}

        [ë‹µë³€]
        {answer}

        [ì‘ì„± ì§€ì¹¨]
        - ë°˜ë“œì‹œ ì„¸ ë¬¸ì¥ ì´ë‚´ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.
        - ë°˜ë“œì‹œ ë‹µë³€ì— ëŒ€í•œ í”¼ë“œë°±ì„ ì œê³µí•©ë‹ˆë‹¤. 
        - ì²« ë²ˆì§¸ ë¬¸ì¥: ë‹µë³€ì˜ ê°•ì ì„ 2~3ê°€ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì¹­ì°¬í•©ë‹ˆë‹¤. (êµ¬ì¡°, ë…¼ë¦¬, í‘œí˜„, ì§ë¬´ ì—°ê´€ì„± ë“±)
        - ë‘ ë²ˆì§¸ ë¬¸ì¥: ê°œì„ í•  ì  1~2ê°€ì§€ë¥¼ êµ¬ì²´ì ì´ê³  ì¹œì ˆí•˜ê²Œ ì œì‹œí•©ë‹ˆë‹¤. (ì˜ˆ: ë” êµ¬ì²´ì ì´ì–´ì•¼ í•œë‹¤, ë…¼ë¦¬ íë¦„ì´ ì•½í•˜ë‹¤ ë“±)
        '''

     